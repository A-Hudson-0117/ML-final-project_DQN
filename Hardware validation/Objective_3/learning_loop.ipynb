{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffb28fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rospy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6636\\2402098052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mDQN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdisc_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDiffDriveEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotLearning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\School\\UNCC\\Fall 2022\\Machine Learning\\Project\\My code\\lab code\\ML Project\\Static objective\\disc_env.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrospy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rospy'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from DQN import Agent\n",
    "from disc_env import DiffDriveEnv\n",
    "from utils import plotLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61364c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"models/\"\n",
    "save_file = \"robot_model_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0364f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(dictonary):\n",
    "    observation = np.zeros(7, dtype=np.float32)\n",
    "    observation[0:5] = np.concatenate([dictonary['agent'], \n",
    "                                       dictonary['target']])\n",
    "    observation[5] = dictonary['heading']\n",
    "    observation[6] = dictonary['iterations']\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e92658",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiffDriveEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c673c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     env.render_mode = \"human\"\n",
    "get = ['agent', 'target', 'heading']\n",
    "agent = Agent(gamma=0.99, epsilon=1, batch_size=512, n_actions=8, eps_end=0.01,input_dims=[7], lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c6328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -49.47 average score -49.47 completed 0 epsilon 1.00\n",
      "episode  50 score 97.20 average score -48.87 completed 2 epsilon 0.01\n",
      "episode  100 score 96.31 average score -37.67 completed 9 epsilon 0.01\n",
      "episode  150 score -71.16 average score -30.32 completed 7 epsilon 0.01\n",
      "episode  200 score -52.67 average score -37.62 completed 4 epsilon 0.01\n",
      "episode  250 score 89.74 average score -34.79 completed 9 epsilon 0.01\n",
      "episode  300 score 100.97 average score -28.48 completed 9 epsilon 0.01\n",
      "episode  350 score -49.55 average score -31.80 completed 6 epsilon 0.01\n",
      "episode  400 score 100.05 average score -26.79 completed 12 epsilon 0.01\n",
      "episode  450 score -82.33 average score -18.49 completed 12 epsilon 0.01\n",
      "episode  500 score -49.61 average score -13.92 completed 14 epsilon 0.01\n",
      "episode  550 score -49.10 average score -8.76 completed 15 epsilon 0.01\n",
      "episode  600 score 100.73 average score -8.50 completed 15 epsilon 0.01\n",
      "episode  650 score -49.05 average score -8.17 completed 15 epsilon 0.01\n",
      "episode  700 score -49.01 average score -13.11 completed 11 epsilon 0.01\n",
      "episode  750 score 100.97 average score -15.00 completed 14 epsilon 0.01\n",
      "episode  800 score -74.54 average score -5.82 completed 17 epsilon 0.01\n",
      "episode  850 score -51.53 average score 2.31 completed 19 epsilon 0.01\n",
      "episode  900 score 100.94 average score -1.92 completed 15 epsilon 0.01\n",
      "episode  950 score 100.79 average score -6.25 completed 16 epsilon 0.01\n",
      "episode  1000 score -51.29 average score -8.45 completed 13 epsilon 0.01\n",
      "episode  1050 score 100.99 average score -9.56 completed 15 epsilon 0.01\n",
      "episode  1100 score 100.96 average score 0.36 completed 20 epsilon 0.01\n",
      "episode  1150 score -49.49 average score 7.99 completed 20 epsilon 0.01\n",
      "episode  1200 score -49.16 average score 3.31 completed 17 epsilon 0.01\n",
      "episode  1250 score -49.00 average score -2.83 completed 16 epsilon 0.01\n",
      "episode  1300 score 101.00 average score -2.76 completed 16 epsilon 0.01\n",
      "episode  1350 score 98.75 average score 10.35 completed 25 epsilon 0.01\n",
      "episode  1400 score -49.02 average score 11.96 completed 17 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/flawless/ML Project/Static objective/learning_loop.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#             score += reward \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         agent\u001b[39m.\u001b[39mstore_transition(observation, action, reward, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                 observation_, done)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         observation \u001b[39m=\u001b[39m observation_\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/flawless/ML%20Project/Static%20objective/learning_loop.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     scores\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/ML Project/Static objective/DQN.py:92\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     90\u001b[0m max_mem \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmem_cntr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmem_size)\n\u001b[0;32m---> 92\u001b[0m batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(max_mem, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     93\u001b[0m batch_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     95\u001b[0m state_batch \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_memory[batch])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, eps_history = [], []\n",
    "n_games = 2000\n",
    "\n",
    "finished = 0\n",
    "for i in range(n_games):\n",
    "    observation = np.zeros(6, dtype=np.float32)\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation, info = env.reset()     \n",
    "    observation = convert_observation(observation)\n",
    "#         for _ in range(50):\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info, completed = env.step(action)\n",
    "        observation_ = convert_observation(observation_)\n",
    "#             score += reward \n",
    "        agent.store_transition(observation, action, reward, \n",
    "                                observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(reward)\n",
    "    if completed: \n",
    "            finished +=1\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    if i % 50 == 0:\n",
    "        print('episode ', i, 'score %.2f' % reward,\n",
    "                'average score %.2f' % avg_score, 'completed %d' % finished,\n",
    "                'epsilon %.2f' % agent.epsilon)\n",
    "        finished = 0\n",
    "x = [i+1 for i in range(n_games)]\n",
    "filename = 'score.png'\n",
    "plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05684402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4380582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_mode = \"human\"\n",
    "observation, info = env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcfcae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desired = 1.441939725935698\n",
      "desired = 0.54277354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'agent': array([-0.37332058, -0.61030316,  0.54277354], dtype=float32),\n",
       "  'target': array([-0.30716679, -0.09975694], dtype=float32),\n",
       "  'heading': 1.441939725935698,\n",
       "  'iterations': 29},\n",
       " -2.6169996426381728,\n",
       " False,\n",
       " {'distance': 0.5767},\n",
       " False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN] [1671041725.385536]: Inbound TCP/IP connection failed: connection from sender terminated before handshake header received. 10 bytes were received. Please check sender for additional details.\n",
      "[WARN] [1671041755.525835]: Inbound TCP/IP connection failed: connection from sender terminated before handshake header received. 10 bytes were received. Please check sender for additional details.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba9896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    observation = convert_observation(observation)\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info, completed = env.step(action)\n",
    "    if completed:\n",
    "        print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd04106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e432e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_mode = \"human\"\n",
    "env.robot_type = \"real\"\n",
    "observation, info = env.reset()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79924532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear: \n",
      "  x: -0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: -0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: -0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: -0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: -0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "linear: \n",
      "  x: 0.12\n",
      "  y: 0.0\n",
      "  z: 0.0\n",
      "angular: \n",
      "  x: 0.0\n",
      "  y: 0.0\n",
      "  z: 0.8\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    observation = convert_observation(observation)\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info, completed = env.step(action)\n",
    "    if completed:\n",
    "        print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b22206",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(agent.state_dict(), save_path + save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e29779",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd03cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_agent = Agent(gamma=0.99, epsilon=1, batch_size=512, n_actions=8, eps_end=0.01,input_dims=[6], lr=0.003)\n",
    "\n",
    "load_agent.load_state_dict(T.load(save_path + save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec5cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
